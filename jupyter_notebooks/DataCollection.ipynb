{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Data Collection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Fetch data from Kaggle and save as raw data\n",
        "* Inspect the data and check for non-image files\n",
        "* Split the data into Train, Test and Validation sets\n",
        "* Save it under inputs/cherry_leaves_dataset/cherry-leaves\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* kaggle.json for the authentication token\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Generate Dataset Folders for sets:\n",
        "  * Train Sets: \n",
        "    * inputs/cherry_leaves_dataset/cherry-leaves/train/healthy\n",
        "    * inputs/cherry_leaves_dataset/cherry-leaves/train/powdery_mildew\n",
        "  * Test Sets: \n",
        "    * inputs/cherry_leaves_dataset/cherry-leaves/test/healthy\n",
        "    * inputs/cherry_leaves_dataset/cherry-leaves/test/powdery_mildew\n",
        "  * Validation Sets: \n",
        "    * inputs/cherry_leaves_dataset/cherry-leaves/validation/healthy\n",
        "    * inputs/cherry_leaves_dataset/cherry-leaves/validation/powdery_mildew\n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* This covers the second and third phases of the CRISP-DM workflow, which are data understanding and data preparation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/alitapantea/Documents/Projects/mildew-detection-project/jupyter_notebooks'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You set a new current directory\n"
          ]
        }
      ],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/alitapantea/Documents/Projects/mildew-detection-project'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Fetch data from Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First we need to install the Kaggle package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# install kaggle package\n",
        "%pip install kaggle==1.5.12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then change the Kaggle configuration directory to the current working directory and set the permissions for the Kaggle authentication file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd()\n",
        "! chmod 600 kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Get the dataset path from the [Kaggle URL](https://www.kaggle.com/codeinstitute/cherry-leaves).\n",
        "* Set your destination folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading cherry-leaves.zip to inputs/cherry_leaves_dataset\n",
            "100%|█████████████████████████████████████▉| 55.0M/55.0M [00:07<00:00, 9.36MB/s]\n",
            "100%|██████████████████████████████████████| 55.0M/55.0M [00:07<00:00, 7.54MB/s]\n"
          ]
        }
      ],
      "source": [
        "KaggleDatasetPath = \"codeinstitute/cherry-leaves\"\n",
        "DestinationFolder = \"inputs/cherry_leaves_dataset\"   \n",
        "! kaggle datasets download -d {KaggleDatasetPath} -p {DestinationFolder}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unzip the downloaded file and then delete it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(DestinationFolder + '/cherry-leaves.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall(DestinationFolder)\n",
        "\n",
        "os.remove(DestinationFolder + '/cherry-leaves.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Check for and remove non-image files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First import os library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_files(dir):\n",
        "    img_ext = ('.png', '.jpg', '.jpeg')\n",
        "    images = 0\n",
        "    non_images = 0\n",
        "    for root, dirs, files in os.walk(dir):\n",
        "        for file in files:\n",
        "            if not file.lower().endswith(img_ext):\n",
        "                filepath = os.path.join(root, file)\n",
        "                os.remove(filepath)\n",
        "                non_images += 1\n",
        "            else:\n",
        "                images += 1\n",
        "    \n",
        "    print(f'Found {non_images} files that were not images')\n",
        "    print(f'Found {images} files that were images')\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 0 files that were not images\n",
            "Found 4208 files that were images\n"
          ]
        }
      ],
      "source": [
        "check_files('inputs/cherry_leaves_dataset/cherry-leaves/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After this step, the dataset should not contain any images. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split Train, Test and Validation set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The next step is to split the images into folders containing the Train, Test and Validation set needed for supervised learning. The folders will also keep the labelling as healthy or powdery_mildew.\n",
        "\n",
        "The following function was taken from the malaria walkthrough project from Code Institute as a basis and adjusted as needed. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import joblib\n",
        "\n",
        "def split_image_sets(dir, train_set_ratio, test_set_ratio, validation_set_ratio):\n",
        "    if train_set_ratio + validation_set_ratio + test_set_ratio != 1.0:\n",
        "        print(\"The ratio of all three sets should sum up to 1.\")\n",
        "        return\n",
        "    \n",
        "    labels = os.listdir(dir) #gets the folder names for healthy/powdery_mildew\n",
        "\n",
        "    if 'test' in labels:\n",
        "        pass\n",
        "    else:\n",
        "        for folder in ['train', 'test', 'validation']:\n",
        "            for label in labels:\n",
        "                os.makedirs(os.path.join(dir, folder, label))\n",
        "        \n",
        "        for label in labels:\n",
        "            files = os.listdir(os.path.join(dir, label))\n",
        "            random.shuffle(files)\n",
        "\n",
        "            train_set_files_qty = int(len(files) * train_set_ratio)\n",
        "            validation_set_files_qty = int(len(files) * validation_set_ratio)\n",
        "\n",
        "            count = 1\n",
        "\n",
        "            for file in files:\n",
        "                if count <= train_set_files_qty:\n",
        "                    shutil.move(os.path.join(dir, label, file), \n",
        "                        os.path.join(dir, 'train', label, file))\n",
        "                elif count <= (train_set_files_qty + validation_set_files_qty):\n",
        "                    shutil.move(os.path.join(dir, label, file), \n",
        "                        os.path.join(dir, 'validation', label, file))\n",
        "                else:\n",
        "                    shutil.move(os.path.join(dir, label, file), \n",
        "                        os.path.join(dir, 'test', label, file))\n",
        "                \n",
        "                count += 1\n",
        "            \n",
        "            os.rmdir(os.path.join(dir, label))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conventionally, the sets are divided as follows:\n",
        "* The training set covers 70% of the data\n",
        "* The test set covers 20% of the data\n",
        "* The validation set covers 10% of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "split_image_sets('inputs/cherry_leaves_dataset/cherry-leaves/', 0.7, 0.2, 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The images are now divided as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 1472 images in train/healthy\n",
            "There are 1472 images in train/powdery_mildew\n",
            "There are 422 images in test/healthy\n",
            "There are 422 images in test/powdery_mildew\n",
            "There are 210 images in validation/healthy\n",
            "There are 210 images in validation/powdery_mildew\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "sets = ['train', 'test', 'validation']\n",
        "labels = ['healthy', 'powdery_mildew']\n",
        "for set in sets:\n",
        "    for label in labels:\n",
        "        number_of_files = len(os.listdir(f'inputs/cherry_leaves_dataset/cherry-leaves/{set}/{label}'))\n",
        "        print(f'There are {number_of_files} images in {set}/{label}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that each set has an even distribution of images across both labels, healthy and powdery_mildew. \n",
        "We can see that the train set has the highest number of images, and that the test set has approximately twice as many as the validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Now that the data is cleaned (there are no non-image files) and the data is split into train, test and validation sets, we can start with the data visualization steps in the next notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
